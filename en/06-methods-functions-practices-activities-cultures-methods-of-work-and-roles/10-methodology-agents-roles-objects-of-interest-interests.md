---
title: 'Methodology: Agents, Roles, Items of Interest, Interests'
---

Thus, in modern systems thinking, we do not emphasize who will play the role of "rational being": a human, a human with enhanced intelligence via a computer, an organization of people and computers, or a computer with artificial intelligence (or artificial intelligence with a computer: it depends on how you look at it). They will all do something similar to what people usually do now, and in an indistinguishable manner (science fiction is very popular with plots where a "legal entity" hides some "artificial intelligence"^[Point 5 in <https://ailev.livejournal.com/1258352.html>]).

If we use the language of work methods and roles working by those methods, rather than specifying the role performers, the nature of the performer does not matter (the construct/material for the functional object does not matter). If we need to obtain an ice cream cone, some performer of the function "selling an ice cream cone" is needed because the cone will not sell itself! It doesn't matter which agent fulfills this function: Petya-the-ice-cream-seller (that's the name of the role for agent Petya!), the vending machine number 4 on the second floor (it won't be called an ice-cream-seller, but "selling an ice cream cone" is one of its functions!), a fast food restaurant (not called an "ice-cream-seller," but they will sell you an ice cream cone!), or even an entire chain of fast food restaurants (also not an "ice-cream-seller," but will fulfill the function of "selling an ice cream cone," and you can even choose the nearest place to get it).

Function is the most stable object under consideration, changing the states of some objects in the surrounding world (cone requested, cone promised, cone being made, cone delivered, ignoring money for now). Next, consider the culturally conditioned role "ice-cream-seller," which for a person is simple, but we can easily call a vending machine performing it (which may be both an ice-cream-seller and a barista, depending on the customer's request), or even a fast food restaurant in that brief moment when it supplies us with an ice cream cone. We can call by role certain other entities unknown to us yet, even though their names are being invented. For example, biocyborg^[<https://www.tandfonline.com/doi/full/10.1080/14636778.2021.2007064>], hybrot^[<https://en.wikipedia.org/wiki/Hybrot>], and so on. At the same time, AGI as artificial general intelligence is often not used as the "intellect" of some being, but as an "intelligent being" without reference to specific embodiment/technical/biological/machine form.

So we will further talk a lot about roles and their functions, and when mentioning "agents" as role performers, these are not always humans. When discussing **roles** and **agents**, they can be not entirely individual humans but sometimes not entirely living entities-objects and sometimes large organizations: the conversation in systems thinking is largely scale-independent and non-anthropocentric. Synonyms for "function" for intellectual and collective agents can also vary—method/practice/culture/style/work, type of labor, type of engineering.

Moving forward, we will mostly mean by agents the agents in their narrow sense, that is, planning intellectual agents or, as also called, **rational agents** (although we won't delve deeply into rationality and decision-making theories, these are topics for other courses^[Details about modern approaches to rationality: <https://ailev.livejournal.com/1619025.html>.]). Rational agents are most commonly humans today, and organizations of people can also be considered in this category, but AI-agents are already showing much rationality.

Usually, rational agents involved in projects aiming to create and develop some systems are interested in a whole range of **"important/interesting characteristics"/"items of interest"** (concern, rarely interest, sometimes even driver as "something important," key/target/main characteristic, object of close attention for some "work method"/practice/activity).

**"Item of interest"/"important characteristic"/"interesting characteristic"** **can be** **any characteristic of the system or project, typical for many projects or unique to this one.** Temperature, development time, adjustability and customizability, mean time between failures: **any important characteristic for a specific project role can be an item of interest.** Cost, performance, reparability, system functions and features, shelf life, safety, etc.: anything important for successfully shifting objects from one state to another by various work methods.

Note that a characteristic or item of interest is not the value of the characteristic! Water temperature can be an item of interest at a pool or beach and is crucial for "water swimming" work methods, but we haven't mentioned the characteristic's value, the temperature value! Roughly speaking, we only named the scale by which we will later judge the value—reparability as an item of interest implies that we will later assess it: whether it's high, medium, low, or other. But "item of interest," "characteristic"—is a property, not the value of the property.

When planning to break down this item of interest into smaller parts, the terms "area of concern" can be used, and in such an area, there will be several specific items of interest. **Area of concern** can be any set of characteristics considered important by work/activity/project/org roles. For instance, a client's area of concern may include system characteristics (what it does and how well), the price of the target system, manufacturing time, and the location for receiving the final system. Again: we haven't said anything about these characteristics' values! We are discussing in terms of scales/properties, not their values.

Note, we just expanded the range of synonyms for "role": adding epithets pointing to different contexts of role usage: project or organizational contexts clarify rational agents—"project role" (role in executing some work methods in the current project), "org-role" (role in executing some organizational methods). Constructs/materials and affordances here are the same as for other roles of more or less universal creators—humans, organization units or entire organizations, teams with their computers and equipment, as well as AI-agents.

Different roles have different **concerns/preferences** **in** **"items of interest"/"important characteristics"**. They are sometimes called **role concerns, role preferences**. This means it's not the preferences of the construct-agent Vasya playing the role of Prince Hamlet but Prince Hamlet’s preferences in his role.

If it's Dasha-the-seller, it's not about Dasha herself but the role "seller" preferences, executing sales methods.

If a common item of interest/important characteristic for project roles buyer and seller is the price, then the buyer’s role concern is minimizing the price, while for the seller, it’s maximizing the price. Therefore we speak of two concepts: characteristic with its value and concern, as for two or more roles, the characteristic may be the same, but the concerns/preferences differ. Don’t confuse: concern/preference as aiming for a specific value on a scale/characteristic/property versus "item of interest"/"item of concern" as the scale/characteristic/property itself.

For example, price: "item of interest" will be the same for all roles negotiating within a project, while the concern/preference (to have a higher or lower price) for each role will differ. Preference/concern is about where to push the characteristic/property value during negotiations: make it higher or lower if continuous, or closer to one value or another if discrete (hot—open the door, everyone is sweating, draft—close the door, I just recovered from a cold, sweating is okay): item of interest—"door openness," different concerns!

People and organizations (and sometimes not only people and their organizations, we won't make these reservations further—be attentive in real life) are considered autonomous **agents** executing **roles**. Sometimes agents as role performers are called **actors**, sometimes **org-unit**(meaning the construct), unlike org-role (functional object, we avoid the word "division" as "org-unit" can mean both the whole enterprise or one person, and some collegiate bodies—councils, commissions generally not considered divisions).

A creating-agent handles "work by method"/activity/labor/practice/engineering/creation, having **intention**^[<https://plato.stanford.edu/entries/intention/>], aimed at eliminating certain dissatisfactions at different system levels, which prompts them to work by a method aimed at changing the world, themselves, or their model of the world. Working by a method, the agent plays a role and pursues role concerns, striving for preferences in certain interesting system characteristics. Intention/intent to act is an element of entrepreneurship/proactivity/research curiosity, evaluating uncertain future and creatively suggesting world changes (for instance, creating new systems) to avoid unpleasant surprises from the universe and prompting creativity.

There's speculation in active inference^[<https://www.activeinference.org/>], which hasn't yet been disproved, hence taken seriously: **all without exception** **systems** **of any scale** **(as stable objects that manage to sustain themselves in the physical world) engage in actions implementing the principle of minimization of free** **energy from physics. Rational agents engage in proactive action, planning possible future unpleasant surprises** **and acting beforehand, sometimes organizing into collectives for more effective action**^[<https://www.sciencedirect.com/science/article/pii/S0149763423004694>]**.**

This applies to molecules, humans, organizations, and so on, up to humanity as a whole. Details can be found in the work by Chris Fields, Karl Friston, James F. Glazebrook, Michael Levin, "A free energy principle for generic quantum systems"^[<https://arxiv.org/abs/2112.15242>], 2021. Of course, in each case, it’s about very different intentions and actions, but due to the physical principle of least action^[https://en.wikipedia.org/wiki/Principle_of_least_action], an agent::system to the extent of its rationality, memory volume (we can always connect computer memory and memory of other people!) and computing power (besides the brain, we can connect other brains and computing resources up to data centers) implements the physical principle of free energy minimization: minimizing expected unpleasant surprises from the universe, i.e., minimizing the chances of disappearing, disintegrating, or dissipating under the pressure of entropy. If you try to break a stone, it will resist as much as it can. If you try to break a person, they can foresee it and either run away or (be warned) hit back—not alone but with friends and using tools/weapons unpleasant for you. How communities and societies react to attempts to destroy them is intriguing, yet answers are tough as they can’t be asked (anyone speaking from their behalf will say different things). It is observed that communities sometimes live shorter than individuals, but sometimes much longer, and humanity vastly longer than individuals, i.e., communities, societies, and humanity also resist cosmic trials, successfully more than individuals. But **speaking about** **communities and societies** **anthropomorphically (e.g., they have intention/intent we can know and consider, societies have interests**) **should be cautious, as** **advanced language for such discussion** **is currently lacking.**

Returning to rational agents like individuals and organizations, intention not only makes them pursue role concerns but often switch roles, changing role concerns. So, it’s crucial to understand items of interest and preferences in them for the project role played by an agent (Bogdan, playing engineer-thermal technician), and also understand the actor's intention as a role player (e.g., Bogdan wants to eat, so his engineer-thermal technician role discussion quickly ends without results, switching to the eater role and heading off to eat).

For realizing agent intentions, strategies are developed based on their understanding of causal relationships in the situation. Broadly, strategy is a continuation of the synonym series method/practice... It’s a pattern of actions, and hence strategy/method of work is essentially the same, barring nuances, "way to achieve success in getting results." Based on strategy (a method without specific time and resource instances), an **action plan** is developed, involving resources for role objects at specific times when resources will be available to execute the strategy/method.

Everything concerning the methods (their creation, comparison, evolution, implementation) is studied under **methodology**, the science of agent work methods. This course offers methodology elements, while the method/work/activity/practice/engineering/strategy concept has additional discussion in a separate methodology course (how to think about world improvement), and general engineering methods (how to change the world for the better) in a systems engineering course, further covering specialized engineering methods in comprehensive (personal engineering, systems management) and applied engineering courses.

People as agent-actors are typically very inventive. Before making choices, they generate and evaluate various decision options, getting creative—offer them a choice between red and blue, and they, adhering to decision-making norms, will wonder what other choices might be available, then unexpectedly choose green and strive to make this option available. Thus, agents can trigger unforeseen action chains even with strictly positive preferences for you.

For example: item of interest: headache, preference/interest: head shouldn't ache, strategy/method of treatment: decapitation, "cut off the head—no head, no headache," a role for this method: executioner with the item of interest/characteristic "presence of head" and preference/interest "no head," then find an agent for the executioner role and execute the decapitation::method in the executor's available time using an effective method studied by the agent. Will this event sequence be realized significantly depends on the agent's rationality and handling the goal (interests) and means (methods/practices) tangle. Not all agents take courses teaching rational decision-making, not just any kind.

For implementing agent intentions, their strategy and plans might involve other agents in different roles executing other methods, possibly even paying them! But our agent might get distracted from their role and take on other roles' work if qualified. If not qualified, they might still do it poorly, possibly unaware of their inadequacies!

This complicates the scenario, but methodological concepts "agent," "actor":: "role performer," "role," "item of interest/important characteristic of a system or project," "interest/preference," "intention," "strategy," "plan" should suffice for thoroughly understanding various situations concerning human behavior in projects, including discussing conflicts and cooperation. The course's goal is to acquaint you with these concepts for managing attention in complex situations with many agents pursuing various intentions driven by different interests. You should find these concept types in life.

Primarily, pay attention to actions (discover the work method, then determine the role, then understand the "items of interest"/"important practice characteristics" of the role, and the role's interest). But also pay attention to words if actions are not visible, but only speeches or texts by the agent "with unclear intentions." Vasya::agent, so in the conversation with me he has some intention and role—what is that intention, what role? And how rational is Vasya, how insane might his actions be in realizing these intentions? I::agent, what are my intentions and roles when communicating with Vasya? After realizing them, what actions should I take?

Unfortunately, methodological considerations are complicated by:

-   Multiple agents/actors usually participate in a situation.
-   Every agent has hidden intentions from other agents (not necessarily humans) in a project.
-   Each agent is involved in multiple projects, and an agent's win in one project may lead to a loss in another, prompting idea generation to optimize the situation, often with multi-level optimization due to conflicts between system levels: a specific item of interest like salary, an employee's interest "raise salary," company's interest "pay the employee less." The agent might increase overall gains for themselves or even others (if collaborating). Thus, an agent could perform multiple roles, some of which you are unaware of, even within different projects, and some conflicting (especially if accountable for different system levels, inevitable conflict between them leading to dissatisfaction from conflicts of all 'bad but equally' solutions).
-   Each agent develops strategies to engage roles in various projects unseen by other agents (including people) in a project, designing an action plan unseen by others. If an agent's strategy fails, they might unexpectedly change strategy and plans, possibly playing entirely different roles with different important characteristics.
-   Each role may have numerous important characteristics/items of interest with preferences/concerns in them.
-   Agents then execute plans (perform works) by the methods/practices well-known to their roles. Here, an agent can be doubly incompetent, as twice unskilled: 1. in their role, playing it poorly if unaware, and 2. lacking intellectual capability, making irrational and inadequate decisions with unknown situations.
-   Agents have authority to manage their own and others' work and equipment (speaking of positions in organizations). Agents might confuse roles of themselves and their subordinates.
-   Here, we don't discuss the methods as a subject of methodology, nor works as a subject of operations management (resource engagement maximizing workflow through these resources, treating the role-executing agent as a resource).


**Translated Text:**