---
title: Concept of Use
---

Knowledge of the existence of different types of systems (supersystems, subsystems) in their relative position to the target system in the system partitioning (mention of system partitioning—this was an indication of the time of use) allows us to more strictly/accurately identify the target system in the world. The concept of a system in physics precisely means some part of the world, separated by a boundary from the rest of the world (environment), and when more talking about descriptions/texts, the word **"context"** is used.

We will distinguish a system from the world with attention, considering **the boundary as the boundary of our attention, and not some material medium.** Thus, we focus on a computer along with its case (the case is not the boundary of the system! The boundary goes where the molecules of the case end and the molecules of the air around the case begin, and this boundary is immaterial, it is "in the mind," it's the boundary of attention), a house along with its outer wall, a cable along with its sheath, a cell along with its membrane.

Next, we introduce the concept of a **"black box"**: it is some system that we imagine without knowing its internal structure—we can only describe the function::behavior of the "black box"::system, manifested at its external boundary, i.e., at the boundary of the space it occupies in the physical world. We know nothing about the internal structure, about the subsystems of the "black box". And if we look inside the system's boundary and talk about how it is arranged, we will call this a **"transparent box"** (sometimes called a "white box"). There is also a **"gray box"**: we know very little about how the system is structured inside its boundary, but we do know something.

We describe the system as a black box at least four times; this is "system consideration":

-   **Functionally:** as a role (functional/role object) and its function in interaction with the environment during operation/use/functioning. A hammer—applies force from the hand to the hammered sharp object.
-   **Constructively:** as the construct we create and develop during creation. A hammer—this one, which we bought in the store (and will implement the hammer during use).
-   **Spatially:** as the place in space that this black box occupies at the time of use. That object lying in the top drawer of the cabinet near the right wall, and at the time of use at workplace number five in room number four.
-   **Cost-wise:** As the total cost of owning the black box. This thing costs 1000 rubles to buy and practically nothing to use.

It is important that all these considerations are about the same object-system and are consistent with each other, that is, they non-contradictorily describe the same system (this is done through 4D extensionalism—it is verified that the described object occupies the same space-time), and they do not delve into the system (in this example, we do not talk about what is inside the hammer—we do not mention its parts: the handle and the hammerhead).

In system consideration, we additionally account for:

-   **Creation graph:** in addition to considering the system as a "black box" at the moment it works, we consider that someone will create and develop this system.
-   **Evolution:** we consider not only the one-time initial creation of the system but also its development: the "black box" will evolve/upgrade, it is not about a one-time event of its appearance. No, there will be an MVP and many increments, **"no version of the system is final."**

"Describing the system" is either:

-   "straightforward engineering," i.e., designing in terms of inventing what kind of system is needed within the supersystem.
-   "reverse engineering" already existing systems if such a description is unavailable but needed for something.

In "straightforward engineering," don't think "we know nothing about the system until it is created, we can't see into the future" (we only mention this because we have heard it from many students). Almost all engineering—is designing systems that do not yet exist, but this does not prevent describing a non-existent system, i.e., hypothesizing about what non-existent system will be successful.
To describe a non-existent system, one must hypothesize what this system will be like in 4D—come up with a system that would perform behavior leading the supersystem to performing some important function in the future. Thought about a building to be constructed next year is quite possible—builders did this even a couple of thousand years ago; there are no difficulties in imagining a system working in the future that hasn't been created yet. In straightforward engineering, they describe a system that does not yet exist in the present, it is the usual case, any design—is hypotheses about what the future will be like!

The very first system consideration—is considering the system as a black box for performing a function ("bringing irreversible benefit") in the supersystem during its operation. The ontological modality of this consideration in both reverse and straightforward engineering is the modality^<https://en.wikipedia.org/wiki/Modal_operator> of belief/doxastic modality, i.e., it is a hypothesis. It can either withstand checking by logic and experiment or not, this is dealt with within **methods** of **engineering justifications**:

-  **Check with logic:** **we show that there are no logical contradictions in the description of the system.** If it is said that our "black box" is white up to mirroring to better reflect sunlight and avoid overheating, and at the same time, it is said that our "black box" is a fashionable dark green color to look aesthetically pleasing, this is a clear contradiction, the description must be changed to eliminate the contradiction (again: it doesn’t matter whether we describe an existing system or a future one). Engineering is about people **agreeing and eliminating contradictions** in such situations.
-   **Check with experiment:** we can take some measurements in the real world and check if these measurements on an already existing (manufactured/embodied) system match the design values for it within some confidence interval.

In any case, both in straightforward and reverse engineering, we invent the description of the "black box" as a hypothesis (we believe it is correct), then criticize this hypothesis and, in case of errors, improve, improve, and improve this hypothesis about the "black box"—work with this description, ensuring its consistency and accuracy in experiments (reverse engineering) and predictions (straightforward engineering, but here there will also be an experiment—creating the system and measuring on the produced system to confirm the hypothesis).

The description in its functional part should demonstrate that the system "brings irreversible benefits" by working within the supersystem, in the constructive part—that it is implementable (can be made!), in the spatial part—that the system can fit where it should operate, in the cost-wise part—that building and operating the system will be profitable.

An "entrepreneurial hypothesis"—this is precisely it, a hypothesis that our "black box" will be useful and cheap, therefore will sell well and investing in its development and production now will yield profits later. This is **visionary work**::method/practice (we avoid using the word "entrepreneurship," although it strictly refers to "Schumpeter's entrepreneur," but few understand "entrepreneurship strictly as Schumpeter," and everyone fantasizes about something of their own. So we taboo the term "entrepreneur," the practice of "entrepreneurship," and break down the everyday understanding into several roles. A **visionary**::role is indeed "Schumpeterian entrepreneur," but this is neither a firm founder, nor a person with a special mindset inclined to risk, nor a rich person, nor an inventor, nothing from the usual associations with "entrepreneur." But this role will evaluate whether the project will be profitable and will hypothesize about it. And if the role thinks there will be no profit—there will be no project).

If we change the modality of the system description of the "black box" from doxastic (belief) to deontic (prohibitions and permissions, prescriptions), the description of the black box is called **system requirements**. Previously, in the end, system requirements were developed, but now they develop **concept of operations** refined and detailed to **use cases**—they primarily differ in this ontological status but not only. **In the concept of operations and further in use cases, they describe the behavior of the system as a "black box," i.e., they describe the system's functions, its role in the environment—** **describing with the status of a hypothesis that this is the correct behavior of a successful system.**

Until about 2015, systems engineering even had a separate method **requirements engineering**, now it doesn't. Check: until 2015, almost a dozen textbooks on variants of the requirements engineering method were published every year, and then it abruptly stopped—by inertia, they still release a book a year, but these are just "old-timers" satisfying the demand of other "old-timers."
More about this transition from requirements engineering to the development of the concept of operations is told in the course "Systems Engineering."

The main points here:

-   Performers of the requirements engineer role (often called **analysts**) stood between developers and external project roles on the ground that "we must not distract developers from development, these developers also don't know how to talk to clients, specially trained people must do it." It turned out that these "specially trained people" (analysts) just created a broken phone situation (the client says one thing, the analyst hears another, documents a third, the developer reads a fourth from the requirements), the harm of which exceeded the harm of "distracting developers from their work," and the time **"reading from the client" and "reading from the requirements"** was about the same. Additionally, there was a time delay in transmitting information through an extra link and a loss of context and justification for the appearance of certain requirements.
-   Requirements arose precisely from concepts of operations that gradually detailed. The most different requirements for certain features were slowly collected into a huge, strictly agreed "monolith" (as architects say now), approved—and then this "monolith" was passed on to development for "unconditional fulfillment." This "collect everything, approve, pass to work" turned out to be terribly slow, although it was much better historically than the previous situation when the system design proceeded without any requirements. Then everything was so bad that it's better not to remember. "Much better with requirements than without them" was the pure truth! But even with requirements, it wasn't so good. The first delay was from reconciling heterogeneous requirements among external project roles, requirements engineers, developers, and architects. These requirements were approved, and everyone understood their great value, given the huge labor invested in them, so they should not be changed—they were agreed upon with such difficulty! Therefore, when obvious errors in the requirements were discovered, or when the situation changed, requiring the requirements to be changed—they preferred not to change the requirements. They made the system inevitably worse than it could have been. The fact that "we have procedures for changing requirements" was fiction and an excuse, these procedures were prohibitively expensive in time, and violating requirements carried punishment, but meeting crooked requirements couldn't be punished, so requirements were sacrosanct.
-   After the requirements reached the developers, they tried to understand what would be truly useful to external project roles, whom the developers had never seen—the analyst had seen them. The problem was not only in the difficulty of changing requirements when errors were found but also that developers themselves believed that they should work with these requirements once, and the tests were planned not to demonstrate the system's fitness for the client (later they figured it out, called acceptance/validation), but to show that "requirements were met" (these tests were called verification). The problem was that the design, development, manufacture, and testing (both verification and validation) were considered one-time actions, which led to the impossibility of improving the system: neither quickly adding a new feature nor quickly excluding an unnecessary one. Not quickly and extremely nervously—it was possible "there are procedures for changing requirements." But quickly—no, it wasn't. Just hear: "we have an incorrect hypothesis, let's quickly fix it" versus "they gave us wrong requirements, let's not follow these requirements." Systems made based on concepts of operations and detailing to use cases **proved to be better** because both the concept of operations and further use cases could somehow be corrected during development, "in working order," if problems were found, while requirements were "approved" and hence could be changed only "in a special, slow, and laborious manner." Well, seasoned old-school engineers didn't mind this "slow and laborious manner," while new-school engineers just smirked and quietly said, "for a crazy dog, seven leagues is not a detour," then demonstrated incredible speeds of development to the "old-timers."
-   Working with concepts of operations and use cases, without developing requirements, used directly by developers, also proceeded faster: various use cases were amended and implemented by different teams of developer-people, while architects monitored the integration of these works. This means teams didn't have to wait for each other until all requirements were gathered and approved into one "monolith." No, everyone expressed their hypotheses in the form of concepts of operations (described the system as a black box), criticized them, checked by creating parts of the system implementing use cases, then improved these use cases—and so the system was created and developed continuously, the bottleneck "collect all requirements together and demand their one-time fulfillment" disappeared, and system creation work was parallelized not only in design but also in understanding what should be designed—what behavior (functions) the "black box" should perform.
-   The refusal of requirements led to the active use of A\|B testing^<https://en.wikipedia.org/wiki/A/B_testing>, where several hypotheses (requirements usually demand something single!) are presented and checked together, then the one found better by some criteria is chosen. **If you have "hypotheses," not "requirements," you handle them differently: not just "meet" but "check and gradually correct."**

The concept of operations (previously requirements developed on its basis, now abandoned) primarily contains information about the system's functions in relation to its working/target/operational/functional environment, so it consists of various models that describe the system's behavior at its boundary in interaction with external systems (systems within the supersystem). The most detailed behavior models are use cases. In some schools of systems engineering, use cases are considered separate from the concept of operations (as they are developed later from concise descriptions of functionality in the concept of operations), in others—they are included in the concept of operations, which gradually changes during the project: it is specified, refined, detailed, including more and more detailed use cases as the system develops. We adopt the second approach: use cases are part of the concept of operations, one of the types of models included. More about this—in the course "Systems Engineering" and the literature offered by the course.

When talking about the concept of operations as a "black box" description, we mean describing the time of use in terms of obtaining the required function from the system. The system **should** **be** during operation in terms of its behavior either meaningfully flashing lights, or providing some computation result, or heating, or leaving a furrow of correct sizes: everything assumed the system will do (hypotheses!) so that at the moment of its use people would say it does what is expected and doesn’t do what is not expected, that is, describing the expected behavior of a successful system. **The expected behavior of the system is** **a prediction of what should happen during use.** **In the case of requirements, it was therefore said functional requirements** **(deontic), but now it is just hypotheses** **of functionality** **(doxastic), so there is no "must,"** **only** **"should be" (and life will show if the hypothesis is justified).**

There were no "non-functional requirements" though the term was often encountered in the literature, usually explained as incorrect to use. More often, they spoke of other types of requirements—like quality requirements (-ilities, types of availability, repairability, reliability), which interest not only the developer but also other project roles, mainly the architect. It is now clear that architecture deals with architectural characteristics (mainly those -ilities), related to the system not in terms of its applied functions but its general behavior at the moment of work/operations (such as reliability characteristics), or even at the time of creation and development (such as ease of change during continuous improvements, evolvability). These characteristics though different, are more or less the same for very different types of systems. For example, scalability: how easily the system’s performance can be increased, if needed—will it just require adding some additional modules (say, adding another wheel for each new ton of the trolley’s capacity, or does everything need to be redesigned from scratch—because increasing load capacity requires changing not only the trolley’s frame and wheels, but everything?) Architectural characteristics describe the system's behavior under unusual conditions or not at the time of operation: ability to work under high load, repairability, affordability (remember, total ownership cost involves both consumables and maintenance during operation but also includes manufacturing costs apart from material costs, affecting systems in the creation chain too), ease of installation. The course "Systems Engineering" deals in detail with the architect’s work, architectural characteristics, ways to achieve acceptable values for metrics measuring architectural characteristics.

Of course, many models intersect both the concept of operations (what the system does at its outer boundary, fitting into surrounding systems at work, bringing some benefit to this world) and architectural characteristics (reliability, ease of modification by developers, destruction resistance, and other characteristics not directly related to the system’s functions) and **system concept** as a description of the internal structure of the system, a "transparent box." As the concept of operations becomes a set of use cases, so the system concept becomes the system design.

If talking about a similar set of descriptions but at the supersystem level (e.g., describing not the concept of operations of a gear in mechanical clocks, but the concept of the clock with the gear in its composition as a system concept), it can be noted that the concept of operations of the gear is essentially part of the clock's concept because the gear’s role in the clock mechanism matters regardless of the gear’s own construction.

Terminology for all these descriptions of the black and transparent boxes can change, especially if looking not at descriptions but at documents expressing these descriptions: easily, the concept of operations of a pump, describing its behavior, can be called a "questionnaire" sent to suppliers with the questions "can you manufacture a pump with these characteristics?" There will be no words "concept of operations" nor the older term "requirements" in the "questionnaire." If there are hints on what should be part of the pump, there won't be words "concept of the system." Architectural characteristics (like reliability) may or may not be indicated, the document's name will still be "questionnaire." You should be able to look at such a "questionnaire"—and determine the type of description: "questionnaire"::"concept of operations" of a pump (and if there are architectural characteristics, note them too).

Why is it necessary to separate the description of the black box as a concept of operations (what the system does externally from the system’s